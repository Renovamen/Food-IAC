# Metrics

This folder contains implementations of some evaluation metrics, some of them are adopted from [ruotianluo/coco-caption](https://github.com/ruotianluo/coco-caption). 


&nbsp;

## List

- **BLEU**: [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf). ACL 2002.
- **CIDEr**: [CIDEr: Consensus-based Image Description Evaluation](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf). CVPR 2015.
- [**METEOR**](http://www.cs.cmu.edu/~alavie/METEOR/)
- **ROUGE-L**: [ROUGE: A Package for Automatic Evaluation of Summaries](https://www.aclweb.org/anthology/W04-1013.pdf). ACL 2004.
- **SPICE**: [SPICE: Semantic Propositional Image Caption Evaluation](https://arxiv.org/abs/1607.08822). ECCV 2016.
- **Novelty** & **Diversity**


&nbsp;

## NOTES

- You will first need to download the [Stanford CoreNLP 3.6.0](http://stanfordnlp.github.io/CoreNLP/index.html) code and models for use by SPICE. To do this, run: `bash get_stanford_models.sh`.
